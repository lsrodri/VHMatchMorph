# Evaluating Visuohaptic Integration on Memory Retention of Morphological Tomographic Images
This repository contains experimental data for the ACM SIGGRAPH VRCAI '24 full paper "Evaluating Visuohaptic Integration on Memory Retention of Morphological Tomographic Images"

# Abstract 
Scientific visualization and tomographic imaging techniques have created unprecedented possibilities for non-destructive analyses of digital specimens in morphology. However, practitioners encounter difficulties retaining critical information from complex tomographic volumes in their workflows. In light of this challenge, we investigated the effectiveness of visuohaptic integration in enhancing memory retention of morphological data. In a within-subjects user study (N=18), participants completed a delayed match-to-sample task, where we compared error rates and response times across visual and visuohaptic sensory modality conditions. Our results indicate that visuohaptic encoding improves the retention of tomographic images, producing significantly reduced error rates and faster response times than its unimodal visual counterpart. Our findings suggest that integrating haptics into scientific visualization interfaces may support professionals in fields such as morphology, where accurate retention of complex spatial data is essential for efficient analysis and decision-making within virtual environments.

## Full Paper
The paper will be available in [ACM Digital Library](https://doi.org/10.1145/3703619.3706055).

## Licenses and Citation

- The [results.csv](results.csv) itself, available in [Creative Commons Public Domain Dedication (CC-0)](https://creativecommons.org/share-your-work/public-domain/cc0/), represents the experimental results from consented anonymous participants and was collected by Lucas Rodrigues.
- The [Data_Analysis.ipynb](Data_Analysis.ipynb) itself, available in [Creative Commons Public Domain Dedication (CC-0)](https://creativecommons.org/share-your-work/public-domain/cc0/), represents the Jupyter Notebook we utilized to analyze our data as described in our article.


<!-- - The [NASA-TLX.csv](NASA-TLX.csv) itself, available in [Creative Commons Public Domain Dedication (CC-0)](https://creativecommons.org/share-your-work/public-domain/cc0/), represented the workload self-assessment results from consented anonymous participants and was collected by Lucas Rodrigues. -->
<!-- - The [MatchToSampleExperiment](MatchToSampleExperiment) itself, available in [Creative Commons Public Domain Dedication (CC-0)](https://creativecommons.org/share-your-work/public-domain/cc0/), represented the open-source Unity project that was created by Lucas Rodrigues and used for data collection. -->

<!-- The contained "source code" (i.e., Python scripts and Jupyter Notebooks) of this work is made available under the terms of [GNU GPLv3](./LICENSE). They are fully available also in the [Open Science Framework](https://). -->

Copyright &copy; 2024. [Cluster of Excellence Matters of Activity](https://www.matters-of-activity.de/). All rights reserved.
